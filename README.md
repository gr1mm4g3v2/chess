# NEURO_CHESS_ZERO

A self-learning chess AI that plays against itself using Q-learning with position memory. Watch two neural networks compete and evolve in real-time.

![Chess AI Interface](https://img.shields.io/badge/Next.js-15-black?logo=next.js) ![TypeScript](https://img.shields.io/badge/TypeScript-5-blue?logo=typescript) ![Socket.io](https://img.shields.io/badge/Socket.io-Realtime-green?logo=socket.io)

## Features

- ğŸ§  **Dual AI Networks** - White and Black AIs compete separately with their own ELO ratings
- ğŸ“š **Q-Learning** - AIs learn from wins/losses, storing up to 10,000 positions per network
- ğŸ¯ **Epsilon-Greedy Exploration** - Balance between trying new moves and exploiting known good ones
- ğŸ“Š **Live Metrics** - Real-time ELO tracking, win/loss/draw stats, and learning progress
- ğŸ“ˆ **Eval Bar** - Visual representation of who's winning
- â±ï¸ **Speed Control** - Adjust training speed from slow (1500ms) to turbo (50ms)
- ğŸ“œ **Game History** - Browse and replay past games move-by-move
- ğŸ’¾ **Persistent Learning** - AI progress saves between sessions

## Getting Started

```bash
# Install dependencies
npm install

# Run the development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to watch the AI train.

## How It Works

### Q-Learning
Each AI maintains a Q-table mapping positions to move values. After each game:
- **Winner's moves** get positive reinforcement (+1.0)
- **Loser's moves** get negative reinforcement (-1.0)
- **Draws** give small positive rewards (+0.1)

### Exploration vs Exploitation
- **Exploration Rate** starts at 30% (try random moves)
- Decays to 5% as the AI gains experience
- Higher exploration = more variety, lower = more optimal play

### Persistence
Two files store AI progress:
- `ai-state.json` - ELO, stats, and Q-tables for both networks
- `game-history.json` - Up to 50 past games for replay

## Tech Stack

- **Frontend**: Next.js 15, React 19, TailwindCSS, Recharts
- **Backend**: Node.js, Socket.io, chess.js
- **AI**: Custom Q-learning implementation

## Project Structure

```
â”œâ”€â”€ server.ts              # Custom server with Socket.io
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/               # Next.js app router
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ CustomBoard.tsx    # SVG chess board
â”‚   â”‚   â”œâ”€â”€ EvalBar.tsx        # Evaluation bar
â”‚   â”‚   â”œâ”€â”€ MetricsPanel.tsx   # Dual AI stats display
â”‚   â”‚   â”œâ”€â”€ SpeedControl.tsx   # Training speed slider
â”‚   â”‚   â””â”€â”€ GameHistory.tsx    # Past games viewer
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ ai/
â”‚       â”‚   â””â”€â”€ neural-net.ts   # Q-learning AI implementation
â”‚       â”œâ”€â”€ game/
â”‚       â”‚   â””â”€â”€ game-manager.ts # Game loop and state management
â”‚       â””â”€â”€ persistence.ts      # Save/load AI state
```

## License

MIT
